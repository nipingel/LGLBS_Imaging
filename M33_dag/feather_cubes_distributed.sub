# combine_images.sub
# script to base submission of LGLBS imaging jobs
#
# Specify the HTCondor Universe (vanilla is the default and is used
#  for almost all jobs) and your desired name of the HTCondor log file,
#  which is where HTCondor will describe what steps it takes to run 
#  your job. Wherever you see $(Cluster), HTCondor will insert the 
#  queue number assigned to this set of jobs at the time of submission.
## universe = docker
## docker_image = nipingel/casa:latest
# Provide HTCondor with the name of your .sif file and universe information
container_image = file:///staging/nmpingel/container_images/casa_container.sif

## feather cube distributed execution
executable = feather_cubes_distributed.sh

## determine channel to feather
sc = $(Process) * 10
ec = $(sc) + 10
start_chan = $INT(sc,%d)
end_chan = $INT(ec,%d)

arguments = $(src_name) $(sdcube) $(interfcube) $(galaxy) $(start_chan) $(end_chan)
output = run_feather_$(src_name)_$(Process).out
error = run_feather_$(src_name)_$(Process).err
log = run_feather_$(src_name)_$(Process).log
# Specify that HTCondor should transfer files to and from the
#  computer where each job runs. The last of these lines *would* be
#  used if there were any other files needed for the executable to use.
Requirements=(Target.HasCHTCStaging == true) 
+HasCHTCStaging=true
+LongJob=true

# Tell HTCondor what amount of compute resources
#  each job will need on the computer where it runs.
transfer_input_files = feather_cubes_distributed.sh, feather_cubes_distributed.py
request_cpus = 1
request_memory = 50GB
request_disk = 10GB
#
# Tell HTCondor to run 1 instances of our job:
queue 64